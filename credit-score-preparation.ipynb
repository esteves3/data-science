{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dslabs_functions import get_variable_types\n",
    "from seaborn import heatmap\n",
    "from dslabs_functions import HEIGHT, plot_multi_scatters_chart\n",
    "from matplotlib.pyplot import figure, subplots, savefig, show, gcf\n",
    "from dslabs_functions import plot_bar_chart\n",
    "from dslabs_functions import set_chart_labels\n",
    "from dslabs_functions import define_grid, HEIGHT\n",
    "from matplotlib.figure import Figure\n",
    "from numpy import ndarray\n",
    "from dslabs_functions import *\n",
    "from pandas import read_csv, DataFrame\n",
    "from numpy import log\n",
    "from pandas import Series\n",
    "from scipy.stats import norm, expon, lognorm\n",
    "from matplotlib.axes import Axes\n",
    "from dslabs_functions import plot_multiline_chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"datasets/class_credit_score.csv\"\n",
    "file_tag = \"credit_score\"\n",
    "data: DataFrame = read_csv(filename, na_values=\"\", index_col=\"ID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option(\"display.max_colwidth\", 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variable encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove non-digits from age column and convert into numeric\n",
    "data['Age'] = data['Age'].str.replace(r'[^0-9]+', '', regex=True)\n",
    "data['Age'] = pd.to_numeric(data['Age'])\n",
    "\n",
    "# Drop name column\n",
    "data = data.drop(columns=['Name'])\n",
    "\n",
    "# Leave only area code for SSN\n",
    "data['SSN'] = data['SSN'].str.slice(stop=3)\n",
    "data = data.rename(columns = {'SSN': 'SSN_Area_Code'})\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def process_loan_type_entry(entry):\n",
    "    if not isinstance(entry, float):\n",
    "        loan_types_split = []\n",
    "        type_list = entry.replace(' and ', ' ')\n",
    "        type_list = type_list.split(', ')\n",
    "        for loan_type in type_list:\n",
    "            loan_types_split.append('Loan_Type_' + loan_type.strip().replace(' ', '_').replace('-', '_'))\n",
    "        return loan_types_split\n",
    "    return np.nan\n",
    "\n",
    "# Split loan types and reformat the strings\n",
    "\n",
    "no_nans = data.dropna()\n",
    "loan_values = no_nans['Type_of_Loan'].unique()\n",
    "\n",
    "loan_types = []\n",
    "for entry in loan_values:\n",
    "    loan_types += process_loan_type_entry(entry)\n",
    "\n",
    "loan_types_columns = set(loan_types)\n",
    "loan_types_columns = list(loan_types_columns)\n",
    "\n",
    "\n",
    "# Create columns and add to dataframe\n",
    "\n",
    "def columns_count_occurrences(column_names, list_to_count):\n",
    "    column_values = dict.fromkeys(column_names, 0)\n",
    "    if isinstance(list_to_count, float):    # nan\n",
    "        for key in column_values:\n",
    "            column_values[key] = np.nan\n",
    "    else:\n",
    "        for item in list_to_count:\n",
    "            column_values[item] += 1\n",
    "    return column_values\n",
    "\n",
    "\n",
    "data[loan_types_columns] = data.apply(lambda row: columns_count_occurrences(loan_types_columns, process_loan_type_entry(row['Type_of_Loan'])), axis='columns', result_type='expand')\n",
    "\n",
    "data = data.drop(columns = ['Type_of_Loan'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can we drop \"num of loan\"?\n",
    "\n",
    "data = data.drop(columns = ['NumofLoan'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert credit history to months\n",
    "\n",
    "import re\n",
    "\n",
    "def convert_age_to_months(age):\n",
    "    if isinstance(age, str):\n",
    "        list_of_numbers = re.findall(r'\\b\\d+\\b', age)\n",
    "        if (len(list_of_numbers) != 2):\n",
    "            print(list_of_numbers)\n",
    "            raise Exception('Incorrect age input')\n",
    "        years, months = int(list_of_numbers[0]), int(list_of_numbers[1])\n",
    "        total_months = years * 12 + months\n",
    "        return total_months\n",
    "    return np.nan\n",
    "\n",
    "data['Credit_History_Age_Months'] = data.apply(lambda row: convert_age_to_months(row['Credit_History_Age']), axis='columns', result_type='expand')\n",
    "data = data.drop(columns=[\"Credit_History_Age\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dummify(df: DataFrame, vars_to_dummify: list[str]) -> DataFrame:\n",
    "    other_vars: list[str] = [c for c in df.columns if not c in vars_to_dummify]\n",
    "\n",
    "    enc = OneHotEncoder(\n",
    "        handle_unknown=\"ignore\", sparse_output=False, dtype=\"bool\", drop=\"if_binary\"\n",
    "    )\n",
    "    trans: ndarray = enc.fit_transform(df[vars_to_dummify])\n",
    "\n",
    "    new_vars: ndarray = enc.get_feature_names_out(vars_to_dummify)\n",
    "\n",
    "    dummy = DataFrame(trans, columns=new_vars, index=df.index)\n",
    "\n",
    "    final_df: DataFrame = concat([df[other_vars], dummy], axis=1)\n",
    "    return final_df\n",
    "\n",
    "def encode_cyclic_variables(data: DataFrame, vars: list[str]):\n",
    "    _data = data\n",
    "    for v in vars:\n",
    "        x_max: float = max(data[v])\n",
    "        _data[v + \"_sin\"] = data[v].apply(lambda x: round(sin(2 * pi * x / x_max), 5))\n",
    "        _data[v + \"_cos\"] = data[v].apply(lambda x: round(cos(2 * pi * x / x_max), 5))\n",
    "    return _data\n",
    "\n",
    "\n",
    "# Substitutions\n",
    "\n",
    "payment_behaviour_enc: dict[str, int] = {\"High_spent_Small_value_payments\": 10, \"Low_spent_Large_value_payments\": 2, \n",
    "                                         \"Low_spent_Medium_value_payments\": 1, \"Low_spent_Small_value_payments\": 0,\n",
    "                                         \"High_spent_Medium_value_payments\": 11, \"High_spent_Large_value_payments\": 12}\n",
    "credit_mix_enc: dict[str, int] = {\"Good\": 2, \"Standard\": 1, \"Bad\": 0}\n",
    "credit_score_enc: dict[str, int] = {\"Good\": 1, \"Poor\": 0}\n",
    "payment_min_amount_enc: dict[str, int] = {\"Yes\": 2, \"NM\": 1, \"No\": 0}\n",
    "month_val: dict[str, float] = {\n",
    "    \"January\": 0,\n",
    "    \"February\": pi / 4,\n",
    "    \"March\": 2 * pi / 4,\n",
    "    \"April\": 3 * pi / 4,\n",
    "    \"May\": pi,\n",
    "    \"June\": - 3 * pi / 4,\n",
    "    \"July\": - 2 * pi / 4,\n",
    "    \"August\": - pi / 4\n",
    "}\n",
    "\n",
    "encoding: dict[str, dict[str, int]] = {\n",
    "    \"Payment_Behaviour\": payment_behaviour_enc,\n",
    "    \"CreditMix\": credit_mix_enc,\n",
    "    \"Month\": month_val,\n",
    "    \"Credit_Score\": credit_score_enc,\n",
    "    \"Payment_of_Min_Amount\": payment_min_amount_enc\n",
    "}\n",
    "\n",
    "\n",
    "# Replace values on dataframe\n",
    "\n",
    "data = data.replace(encoding, inplace=False)\n",
    "\n",
    "vars = [\"Occupation\"]\n",
    "data = dummify(data, vars)\n",
    "\n",
    "data['Customer_ID'] = data.apply(lambda row: int(row['Customer_ID'].replace('CUS_', ''), 16), axis='columns', result_type='expand')\n",
    "\n",
    "data = encode_cyclic_variables(data, [\"Month\"])\n",
    "data = data.drop(columns=['Month'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing value imputation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2nd approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 1: dropping MVs (records and variables)\n",
    "\n",
    "data_mvi_1 = data.copy()\n",
    "\n",
    "def mvi_by_dropping(\n",
    "    data: DataFrame, min_pct_per_variable: float = 0.1, min_pct_per_record: float = 0.0\n",
    ") -> DataFrame:\n",
    "    # Deleting variables\n",
    "    df: DataFrame = data.dropna(\n",
    "        axis=1, thresh=data.shape[0] * min_pct_per_variable, inplace=False\n",
    "    )\n",
    "    # Deleting records\n",
    "    df.dropna(axis=0, thresh=data.shape[1] * min_pct_per_record, inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "cols_before_drop = data_mvi_1.columns\n",
    "\n",
    "# \"thresh=N requires that a column has at least N non-NaNs to survive\"\n",
    "data_mvi_1 = mvi_by_dropping(data_mvi_1, min_pct_per_variable=0.1, min_pct_per_record=0.1)\n",
    "\n",
    "print('Removed columns:')\n",
    "print(list(set(cols_before_drop) - set(data_mvi_1.columns)))\n",
    "print(data_mvi_1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 2: filling MVs\n",
    "\n",
    "from numpy import ndarray\n",
    "from pandas import concat\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from dslabs_functions import get_variable_types, mvi_by_filling\n",
    "\n",
    "def mvi_by_filling_per_column(data: DataFrame, cols: list, strategy: str = \"mean\") -> Series:\n",
    "    # Possible strategies: \"mean\", \"median\", \"most_frequent\"\n",
    "    if strategy != \"knn\":\n",
    "        imp = SimpleImputer(strategy=strategy, copy=True)\n",
    "        df = DataFrame(\n",
    "            imp.fit_transform(data[cols]),\n",
    "            columns=cols,\n",
    "        )\n",
    "        return df[cols]\n",
    "    else:\n",
    "        imp = KNNImputer(n_neighbors=5)\n",
    "        imp.fit(data)\n",
    "        ar: ndarray = imp.transform(data)\n",
    "        df = DataFrame(ar, columns=data.columns, index=data.index)\n",
    "        return df\n",
    "\n",
    "df_to_change = data.copy()\n",
    "\n",
    "df = mvi_by_filling_per_column(df_to_change, [], 'knn')\n",
    "\n",
    "knn_cols = ['SSN_Area_Code', 'CreditMix', 'Amountinvestedmonthly', 'MonthlyBalance']\n",
    "data_mvi_1[knn_cols] = df[knn_cols].copy()\n",
    "\n",
    "knn_cols_3 = ['SSN_Area_Code', 'CreditMix', 'Amountinvestedmonthly', 'MonthlyBalance', 'Monthly_Inhand_Salary',\n",
    "              'NumofDelayedPayment', 'ChangedCreditLimit', 'NumCreditInquiries', 'Credit_History_Age_Months', 'Payment_Behaviour']\n",
    "data_mvi_3 = df.copy()\n",
    "data_mvi_3[knn_cols_3] = df[knn_cols_3]\n",
    "\n",
    "\n",
    "print(data_mvi_1.describe())\n",
    "\n",
    "median_cols = ['Monthly_Inhand_Salary', 'NumofDelayedPayment', 'ChangedCreditLimit', 'NumCreditInquiries', 'Credit_History_Age_Months']\n",
    "data_mvi_1[median_cols] = mvi_by_filling_per_column(data_mvi_1, median_cols, 'median').values\n",
    "\n",
    "mode_cols = ['Payment_Behaviour']\n",
    "data_mvi_1[mode_cols] = mvi_by_filling_per_column(data_mvi_1, mode_cols, 'most_frequent').values\n",
    "\n",
    "# Type of loan\n",
    "data_mvi_1[loan_types_columns] = mvi_by_filling_per_column(data_mvi_1, loan_types_columns, 'most_frequent').values\n",
    "\n",
    "# Occupations\n",
    "occupation_columns = ['Occupation_Accountant', 'Occupation_Architect', 'Occupation_Developer', 'Occupation_Doctor',\n",
    "                      'Occupation_Engineer', 'Occupation_Entrepreneur', 'Occupation_Journalist', 'Occupation_Lawyer',\n",
    "                       'Occupation_Manager', 'Occupation_Mechanic', 'Occupation_Media_Manager', 'Occupation_Musician',\n",
    "                       'Occupation_Scientist', 'Occupation_Teacher', 'Occupation_Writer']\n",
    "\n",
    "occupation_perc = []\n",
    "for occ in occupation_columns:\n",
    "    occupation_count = (data_mvi_1[occ] == True).sum()\n",
    "    total_occupation = (data_mvi_1['Occupation_nan'] == False).sum()\n",
    "    occupation_perc.append(occupation_count / total_occupation)\n",
    "\n",
    "def choose_from_probability(column_names, probabilities):\n",
    "    column_values = dict.fromkeys(column_names, False)\n",
    "    chosen = np.random.choice(column_names, p=probabilities)\n",
    "    column_values[chosen] = True\n",
    "    return column_values\n",
    "\n",
    "data_mvi_1[occupation_columns] = data_mvi_1.apply(lambda row: row[occupation_columns] if row['Occupation_nan'] == False else choose_from_probability(occupation_columns, occupation_perc), axis='columns', result_type='expand')\n",
    "data_mvi_1 = data_mvi_1.drop(columns = ['Occupation_nan'])\n",
    "\n",
    "print(data_mvi_1.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_mvi_3[occupation_columns] = data_mvi_3.apply(lambda row: row[occupation_columns] if row['Occupation_nan'] == False else choose_from_probability(occupation_columns, occupation_perc), axis='columns', result_type='expand')\n",
    "data_mvi_3 = data_mvi_3.drop(columns = ['Occupation_nan'])\n",
    "data_mvi_3[loan_types_columns] = mvi_by_filling_per_column(data_mvi_3, loan_types_columns, 'most_frequent').values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparison between the two approaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import ndarray\n",
    "from pandas import DataFrame, read_csv\n",
    "from matplotlib.pyplot import savefig, show, figure\n",
    "from dslabs_functions import plot_multibar_chart, CLASS_EVAL_METRICS, run_NB, run_KNN\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def evaluate_approach(\n",
    "    train: DataFrame, test: DataFrame, target: str = \"class\", metric: str = \"accuracy\"\n",
    ") -> dict[str, list]:\n",
    "    trnY = train.pop(target).values\n",
    "    trnX: ndarray = train.values\n",
    "    tstY = test.pop(target).values\n",
    "    tstX: ndarray = test.values\n",
    "    eval: dict[str, list] = {}\n",
    "\n",
    "    #eval_NB: dict[str, float] = run_NB(trnX, trnY, tstX, tstY, metric=metric)\n",
    "    eval_KNN: dict[str, float] = run_KNN(trnX, trnY, tstX, tstY, metric=metric)\n",
    "    if eval_KNN != {}:\n",
    "        for met in CLASS_EVAL_METRICS:\n",
    "            eval[met] = [eval_KNN[met]]\n",
    "    return eval\n",
    "\n",
    "\n",
    "file_tag = 'credit_score_mvi_1'\n",
    "target = 'Credit_Score'\n",
    "y = data_mvi_1[target]\n",
    "X = data_mvi_1.drop(columns = target)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "train = pd.concat([X_train, y_train], axis=1)\n",
    "test = pd.concat([X_test, y_test], axis=1)\n",
    "\n",
    "\n",
    "figure()\n",
    "eval: dict[str, list] = evaluate_approach(train, test, target=target, metric=\"recall\")\n",
    "plot_multibar_chart(\n",
    "    [\"NB\", \"KNN\"], eval, title=f\"{file_tag} evaluation\", percentage=True\n",
    ")\n",
    "savefig(f\"images/{file_tag}_eval.png\")\n",
    "show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_tag = 'credit_score_mvi_3'\n",
    "target = 'Credit_Score'\n",
    "y = data_mvi_3[target]\n",
    "X = data_mvi_3.drop(columns = target)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "train = pd.concat([X_train, y_train], axis=1)\n",
    "test = pd.concat([X_test, y_test], axis=1)\n",
    "\n",
    "\n",
    "figure()\n",
    "eval: dict[str, list] = evaluate_approach(train, test, target=target, metric=\"recall\")\n",
    "plot_multibar_chart(\n",
    "    [\"NB\", \"KNN\"], eval, title=f\"{file_tag} evaluation\", percentage=True\n",
    ")\n",
    "savefig(f\"images/{file_tag}_eval.png\")\n",
    "show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outliers treatment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Original data: {data_mvi_3.shape}\")\n",
    "\n",
    "n_std: int = NR_STDEV\n",
    "numeric_vars: list[str] = get_variable_types(data_mvi_3)[\"numeric\"]\n",
    "def evaluate_approach(\n",
    "    train: DataFrame, test: DataFrame, target: str = \"class\", metric: str = \"accuracy\"\n",
    ") -> dict[str, list]:\n",
    "    trnY = train.pop(target).values\n",
    "    trnX: ndarray = train.values\n",
    "    tstY = test.pop(target).values\n",
    "    tstX: ndarray = test.values\n",
    "    eval: dict[str, list] = {}\n",
    "\n",
    "    #eval_NB: dict[str, float] = run_NB(trnX, trnY, tstX, tstY, metric=metric)\n",
    "    eval_KNN: dict[str, float] = run_KNN(trnX, trnY, tstX, tstY, metric=metric)\n",
    "    if eval_KNN != {}:\n",
    "        for met in CLASS_EVAL_METRICS:\n",
    "            eval[met] = [eval_KNN[met]]\n",
    "    return eval\n",
    "\n",
    "\n",
    "for dataset in [\"mvi_1\", \"mvi_3\"]:\n",
    "# Alternative 1: Dropping Outliers\n",
    "\n",
    "    df: DataFrame = data_mvi_3.copy(deep=True) if dataset == 'mvi_3' else data_mvi_1.copy(deep=True)\n",
    "    file_tag = f'credit_score_outliers_{dataset}'\n",
    "    target = 'Credit_Score'\n",
    "    if numeric_vars is not None:\n",
    "        \n",
    "        summary5: DataFrame = data_mvi_3[numeric_vars].describe()\n",
    "        for var in numeric_vars:\n",
    "            top_threshold, bottom_threshold = determine_outlier_thresholds_for_var(\n",
    "                summary5[var],\n",
    "                std_based=False\n",
    "            )\n",
    "            outliers: Series = df[(df[var] > top_threshold) | (df[var] < bottom_threshold)]\n",
    "            df.drop(outliers.index, axis=0, inplace=True)\n",
    "        print(f\"Data after dropping outliers: {df.shape}\")\n",
    "\n",
    "        \n",
    "        y = df[target]\n",
    "        X = df.drop(columns = target)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "        train = pd.concat([X_train, y_train], axis=1)\n",
    "        test = pd.concat([X_test, y_test], axis=1)\n",
    "\n",
    "\n",
    "        figure()\n",
    "        eval: dict[str, list] = evaluate_approach(train, test, target=target, metric=\"recall\")\n",
    "        plot_multibar_chart(\n",
    "            [\"KNN\"], eval, title=f\"{file_tag} evaluation\", percentage=True\n",
    "        )\n",
    "        savefig(f\"images/{file_tag}_drop_iqr_eval.png\")\n",
    "        show()\n",
    "    else:\n",
    "        print(\"There are no numeric variables\")\n",
    "\n",
    "    # Alternative 2: Replacing outliers with fixed value\n",
    "\n",
    "    df: DataFrame = data_mvi_3.copy(deep=True) if dataset == 'mvi_3' else data_mvi_1.copy(deep=True)\n",
    "    if [] != numeric_vars:\n",
    "        for var in numeric_vars:\n",
    "            top, bottom = determine_outlier_thresholds_for_var(summary5[var], std_based=False)\n",
    "            median: float = df[var].median()\n",
    "            df[var] = df[var].apply(lambda x: median if x > top or x < bottom else x)\n",
    "        print(\"Data after replacing outliers:\", df.shape)\n",
    "\n",
    "        y = df[target]\n",
    "        X = df.drop(columns = target)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "        train = pd.concat([X_train, y_train], axis=1)\n",
    "        test = pd.concat([X_test, y_test], axis=1)\n",
    "\n",
    "\n",
    "        figure()\n",
    "        eval: dict[str, list] = evaluate_approach(train, test, target=target, metric=\"recall\")\n",
    "        plot_multibar_chart(\n",
    "            [\"KNN\"], eval, title=f\"{file_tag} evaluation\", percentage=True\n",
    "        )\n",
    "        savefig(f\"images/{file_tag}_replace_iqr_eval.png\")\n",
    "        show()\n",
    "    else:\n",
    "        print(\"There are no numeric variables\")\n",
    "\n",
    "\n",
    "    # Alternative 3: Truncating outliers\n",
    "    df: DataFrame = data_mvi_3.copy(deep=True) if dataset == 'mvi_3' else data_mvi_1.copy(deep=True)\n",
    "    if [] != numeric_vars:\n",
    "        for var in numeric_vars:\n",
    "            top, bottom = determine_outlier_thresholds_for_var(summary5[var], std_based=False)\n",
    "            df[var] = df[var].apply(\n",
    "                lambda x: top if x > top else bottom if x < bottom else x\n",
    "            )\n",
    "        print(\"Data after truncating outliers:\", df.shape)\n",
    "\n",
    "        y = df[target]\n",
    "        X = df.drop(columns = target)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "        train = pd.concat([X_train, y_train], axis=1)\n",
    "        test = pd.concat([X_test, y_test], axis=1)\n",
    "\n",
    "\n",
    "        figure()\n",
    "        eval: dict[str, list] = evaluate_approach(train, test, target=target, metric=\"recall\")\n",
    "        plot_multibar_chart(\n",
    "            [\"KNN\"], eval, title=f\"{file_tag} evaluation\", percentage=True\n",
    "        )\n",
    "        savefig(f\"images/{file_tag}_trunc_iqr_eval.png\")\n",
    "        show()\n",
    "    else:\n",
    "        print(\"There are no numeric variables\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Original data: {data_mvi_3.shape}\")\n",
    "\n",
    "n_std: int = NR_STDEV\n",
    "numeric_vars: list[str] = get_variable_types(data_mvi_3)[\"numeric\"]\n",
    "def evaluate_approach(\n",
    "    train: DataFrame, test: DataFrame, target: str = \"class\", metric: str = \"accuracy\"\n",
    ") -> dict[str, list]:\n",
    "    trnY = train.pop(target).values\n",
    "    trnX: ndarray = train.values\n",
    "    tstY = test.pop(target).values\n",
    "    tstX: ndarray = test.values\n",
    "    eval: dict[str, list] = {}\n",
    "\n",
    "    #eval_NB: dict[str, float] = run_NB(trnX, trnY, tstX, tstY, metric=metric)\n",
    "    eval_KNN: dict[str, float] = run_KNN(trnX, trnY, tstX, tstY, metric=metric)\n",
    "    if eval_KNN != {}:\n",
    "        for met in CLASS_EVAL_METRICS:\n",
    "            eval[met] = [eval_KNN[met]]\n",
    "    return eval\n",
    "\n",
    "\n",
    "for dataset in [\"mvi_1\", \"mvi_3\"]:\n",
    "# Alternative 1: Dropping Outliers\n",
    "\n",
    "    df: DataFrame = data_mvi_3.copy(deep=True) if dataset == 'mvi_3' else data_mvi_1.copy(deep=True)\n",
    "    file_tag = f'credit_score_outliers_{dataset}'\n",
    "    target = 'Credit_Score'\n",
    "    if numeric_vars is not None:\n",
    "        \n",
    "        summary5: DataFrame = data_mvi_3[numeric_vars].describe()\n",
    "        for var in numeric_vars:\n",
    "            top_threshold, bottom_threshold = determine_outlier_thresholds_for_var(\n",
    "                summary5[var],\n",
    "            )\n",
    "            outliers: Series = df[(df[var] > top_threshold) | (df[var] < bottom_threshold)]\n",
    "            df.drop(outliers.index, axis=0, inplace=True)\n",
    "        print(f\"Data after dropping outliers: {df.shape}\")\n",
    "\n",
    "        \n",
    "        y = df[target]\n",
    "        X = df.drop(columns = target)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "        train = pd.concat([X_train, y_train], axis=1)\n",
    "        test = pd.concat([X_test, y_test], axis=1)\n",
    "\n",
    "\n",
    "        figure()\n",
    "        eval: dict[str, list] = evaluate_approach(train, test, target=target, metric=\"recall\")\n",
    "        plot_multibar_chart(\n",
    "            [\"KNN\"], eval, title=f\"{file_tag} evaluation\", percentage=True\n",
    "        )\n",
    "        savefig(f\"images/{file_tag}_drop_std_dev_eval.png\")\n",
    "        show()\n",
    "    else:\n",
    "        print(\"There are no numeric variables\")\n",
    "\n",
    "    # Alternative 2: Replacing outliers with fixed value\n",
    "\n",
    "    df: DataFrame = data_mvi_3.copy(deep=True) if dataset == 'mvi_3' else data_mvi_1.copy(deep=True)\n",
    "    if [] != numeric_vars:\n",
    "        for var in numeric_vars:\n",
    "            top, bottom = determine_outlier_thresholds_for_var(summary5[var])\n",
    "            median: float = df[var].median()\n",
    "            df[var] = df[var].apply(lambda x: median if x > top or x < bottom else x)\n",
    "        print(\"Data after replacing outliers:\", df.shape)\n",
    "\n",
    "        y = df[target]\n",
    "        X = df.drop(columns = target)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "        train = pd.concat([X_train, y_train], axis=1)\n",
    "        test = pd.concat([X_test, y_test], axis=1)\n",
    "\n",
    "\n",
    "        figure()\n",
    "        eval: dict[str, list] = evaluate_approach(train, test, target=target, metric=\"recall\")\n",
    "        plot_multibar_chart(\n",
    "            [\"KNN\"], eval, title=f\"{file_tag} evaluation\", percentage=True\n",
    "        )\n",
    "        savefig(f\"images/{file_tag}_replace_std_dev_eval.png\")\n",
    "        show()\n",
    "    else:\n",
    "        print(\"There are no numeric variables\")\n",
    "\n",
    "\n",
    "    # Alternative 3: Truncating outliers\n",
    "    df: DataFrame = data_mvi_3.copy(deep=True) if dataset == 'mvi_3' else data_mvi_1.copy(deep=True)\n",
    "    if [] != numeric_vars:\n",
    "        for var in numeric_vars:\n",
    "            top, bottom = determine_outlier_thresholds_for_var(summary5[var], std_based=False)\n",
    "            df[var] = df[var].apply(\n",
    "                lambda x: top if x > top else bottom if x < bottom else x\n",
    "            )\n",
    "        print(\"Data after truncating outliers:\", df.shape)\n",
    "\n",
    "        y = df[target]\n",
    "        X = df.drop(columns = target)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "        train = pd.concat([X_train, y_train], axis=1)\n",
    "        test = pd.concat([X_test, y_test], axis=1)\n",
    "\n",
    "\n",
    "        figure()\n",
    "        eval: dict[str, list] = evaluate_approach(train, test, target=target, metric=\"recall\")\n",
    "        plot_multibar_chart(\n",
    "            [\"KNN\"], eval, title=f\"{file_tag} evaluation\", percentage=True\n",
    "        )\n",
    "        savefig(f\"images/{file_tag}_trunc_std_dev_eval.png\")\n",
    "        show()\n",
    "    else:\n",
    "        print(\"There are no numeric variables\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Assign the best to the best dataset mvi\n",
    "data_mvi_3_outliers_replace = data_mvi_3.copy()\n",
    "summary5: DataFrame = data_mvi_3_outliers_replace[numeric_vars].describe()\n",
    "if [] != numeric_vars:\n",
    "    for var in numeric_vars:\n",
    "        top, bottom = determine_outlier_thresholds_for_var(summary5[var], std_based=False)\n",
    "        median: float = data_mvi_3_outliers_replace[var].median()\n",
    "        data_mvi_3[var] = data_mvi_3_outliers_replace[var].apply(lambda x: median if x > top or x < bottom else x)\n",
    "    print(\"Data after replacing outliers:\", data_mvi_3_outliers_replace.shape)\n",
    "    print(data_mvi_3_outliers_replace.head())\n",
    "else:\n",
    "    print(\"There are no numeric variables\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import read_csv, DataFrame, Series\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def evaluate_approach(\n",
    "    train: DataFrame, test: DataFrame, target: str = \"class\", metric: str = \"accuracy\"\n",
    ") -> dict[str, list]:\n",
    "    trnY = train.pop(target).values\n",
    "    trnX: ndarray = train.values\n",
    "    tstY = test.pop(target).values\n",
    "    tstX: ndarray = test.values\n",
    "    eval: dict[str, list] = {}\n",
    "\n",
    "    eval_NB: dict[str, float] = run_NB(trnX, trnY, tstX, tstY, metric=metric)\n",
    "    eval_KNN: dict[str, float] = run_KNN(trnX, trnY, tstX, tstY, metric=metric)\n",
    "    if eval_NB != {} and eval_KNN != {}:\n",
    "        for met in CLASS_EVAL_METRICS:\n",
    "            eval[met] = [eval_NB[met], eval_KNN[met]]\n",
    "    return eval\n",
    "\n",
    "data_to_eval = data_mvi_3.copy()\n",
    "\n",
    "target = \"Credit_Score\"\n",
    "\n",
    "vars: list[str] = data_to_eval.columns.to_list()\n",
    "target_data: Series = data_to_eval.pop(target)\n",
    "\n",
    "file_tag = f'credit_score_scalling_mvi_3_'\n",
    "\n",
    "#Alternative 1: Standard Scalar\n",
    "transf: StandardScaler = StandardScaler(with_mean=True, with_std=True, copy=True).fit(\n",
    "    data_to_eval\n",
    ")\n",
    "df_zscore = DataFrame(transf.transform(data_to_eval), index=data_to_eval.index)\n",
    "df_zscore[target] = target_data\n",
    "df_zscore.columns = vars\n",
    "\n",
    "y = df_zscore[target]\n",
    "X = df_zscore.drop(columns = target)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "train = pd.concat([X_train, y_train], axis=1)\n",
    "test = pd.concat([X_test, y_test], axis=1)\n",
    "\n",
    "\n",
    "figure()\n",
    "eval: dict[str, list] = evaluate_approach(train, test, target=target, metric=\"recall\")\n",
    "plot_multibar_chart(\n",
    "    [\"NB\", \"KNN\"], eval, title=f\"{file_tag} evaluation\", percentage=True\n",
    ")\n",
    "savefig(f\"images/{file_tag}std_scalar_eval.png\")\n",
    "show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Balancing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
