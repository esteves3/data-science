{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dslabs_functions import get_variable_types\n",
    "from seaborn import heatmap\n",
    "from dslabs_functions import HEIGHT, plot_multi_scatters_chart\n",
    "from matplotlib.pyplot import figure, subplots, savefig, show, gcf\n",
    "from dslabs_functions import plot_bar_chart\n",
    "from dslabs_functions import set_chart_labels\n",
    "from dslabs_functions import define_grid, HEIGHT\n",
    "from matplotlib.figure import Figure\n",
    "from numpy import ndarray\n",
    "from dslabs_functions import *\n",
    "from pandas import read_csv, DataFrame\n",
    "from numpy import log\n",
    "from pandas import Series\n",
    "from scipy.stats import norm, expon, lognorm\n",
    "from matplotlib.axes import Axes\n",
    "from dslabs_functions import plot_multiline_chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"datasets/class_credit_score.csv\"\n",
    "file_tag = \"credit_score\"\n",
    "data: DataFrame = read_csv(filename, na_values=\"\", index_col=\"ID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option(\"display.max_colwidth\", 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variable encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Customer_ID</th>\n",
       "      <th>Month</th>\n",
       "      <th>Age</th>\n",
       "      <th>SSN_Area_Code</th>\n",
       "      <th>Occupation</th>\n",
       "      <th>Annual_Income</th>\n",
       "      <th>Monthly_Inhand_Salary</th>\n",
       "      <th>Num_Bank_Accounts</th>\n",
       "      <th>Num_Credit_Card</th>\n",
       "      <th>Interest_Rate</th>\n",
       "      <th>NumofLoan</th>\n",
       "      <th>Type_of_Loan</th>\n",
       "      <th>Delay_from_due_date</th>\n",
       "      <th>NumofDelayedPayment</th>\n",
       "      <th>ChangedCreditLimit</th>\n",
       "      <th>NumCreditInquiries</th>\n",
       "      <th>CreditMix</th>\n",
       "      <th>OutstandingDebt</th>\n",
       "      <th>CreditUtilizationRatio</th>\n",
       "      <th>Credit_History_Age</th>\n",
       "      <th>Payment_of_Min_Amount</th>\n",
       "      <th>TotalEMIpermonth</th>\n",
       "      <th>Amountinvestedmonthly</th>\n",
       "      <th>Payment_Behaviour</th>\n",
       "      <th>MonthlyBalance</th>\n",
       "      <th>Credit_Score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0x1602</th>\n",
       "      <td>CUS_0xd40</td>\n",
       "      <td>January</td>\n",
       "      <td>23</td>\n",
       "      <td>821</td>\n",
       "      <td>Scientist</td>\n",
       "      <td>19114.12</td>\n",
       "      <td>1824.843333</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>Auto Loan, Credit-Builder Loan, Personal Loan, and Home Equity Loan</td>\n",
       "      <td>3</td>\n",
       "      <td>7.0</td>\n",
       "      <td>11.27</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>809.98</td>\n",
       "      <td>26.822620</td>\n",
       "      <td>22 Years and 1 Months</td>\n",
       "      <td>No</td>\n",
       "      <td>49.574949</td>\n",
       "      <td>80.415295</td>\n",
       "      <td>High_spent_Small_value_payments</td>\n",
       "      <td>312.494089</td>\n",
       "      <td>Good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0x1603</th>\n",
       "      <td>CUS_0xd40</td>\n",
       "      <td>February</td>\n",
       "      <td>23</td>\n",
       "      <td>821</td>\n",
       "      <td>Scientist</td>\n",
       "      <td>19114.12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>Auto Loan, Credit-Builder Loan, Personal Loan, and Home Equity Loan</td>\n",
       "      <td>-1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11.27</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Good</td>\n",
       "      <td>809.98</td>\n",
       "      <td>31.944960</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No</td>\n",
       "      <td>49.574949</td>\n",
       "      <td>118.280222</td>\n",
       "      <td>Low_spent_Large_value_payments</td>\n",
       "      <td>284.629163</td>\n",
       "      <td>Good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0x1604</th>\n",
       "      <td>CUS_0xd40</td>\n",
       "      <td>March</td>\n",
       "      <td>500</td>\n",
       "      <td>821</td>\n",
       "      <td>Scientist</td>\n",
       "      <td>19114.12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>Auto Loan, Credit-Builder Loan, Personal Loan, and Home Equity Loan</td>\n",
       "      <td>3</td>\n",
       "      <td>7.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Good</td>\n",
       "      <td>809.98</td>\n",
       "      <td>28.609352</td>\n",
       "      <td>22 Years and 3 Months</td>\n",
       "      <td>No</td>\n",
       "      <td>49.574949</td>\n",
       "      <td>81.699521</td>\n",
       "      <td>Low_spent_Medium_value_payments</td>\n",
       "      <td>331.209863</td>\n",
       "      <td>Good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0x1605</th>\n",
       "      <td>CUS_0xd40</td>\n",
       "      <td>April</td>\n",
       "      <td>23</td>\n",
       "      <td>821</td>\n",
       "      <td>Scientist</td>\n",
       "      <td>19114.12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>Auto Loan, Credit-Builder Loan, Personal Loan, and Home Equity Loan</td>\n",
       "      <td>5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.27</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Good</td>\n",
       "      <td>809.98</td>\n",
       "      <td>31.377862</td>\n",
       "      <td>22 Years and 4 Months</td>\n",
       "      <td>No</td>\n",
       "      <td>49.574949</td>\n",
       "      <td>199.458074</td>\n",
       "      <td>Low_spent_Small_value_payments</td>\n",
       "      <td>223.451310</td>\n",
       "      <td>Good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0x1606</th>\n",
       "      <td>CUS_0xd40</td>\n",
       "      <td>May</td>\n",
       "      <td>23</td>\n",
       "      <td>821</td>\n",
       "      <td>Scientist</td>\n",
       "      <td>19114.12</td>\n",
       "      <td>1824.843333</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>Auto Loan, Credit-Builder Loan, Personal Loan, and Home Equity Loan</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11.27</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Good</td>\n",
       "      <td>809.98</td>\n",
       "      <td>24.797347</td>\n",
       "      <td>22 Years and 5 Months</td>\n",
       "      <td>No</td>\n",
       "      <td>49.574949</td>\n",
       "      <td>41.420153</td>\n",
       "      <td>High_spent_Medium_value_payments</td>\n",
       "      <td>341.489231</td>\n",
       "      <td>Good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0x25fe9</th>\n",
       "      <td>CUS_0x942c</td>\n",
       "      <td>April</td>\n",
       "      <td>25</td>\n",
       "      <td>078</td>\n",
       "      <td>Mechanic</td>\n",
       "      <td>39628.99</td>\n",
       "      <td>3359.415833</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>Auto Loan, and Student Loan</td>\n",
       "      <td>23</td>\n",
       "      <td>7.0</td>\n",
       "      <td>11.50</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>502.38</td>\n",
       "      <td>34.663572</td>\n",
       "      <td>31 Years and 6 Months</td>\n",
       "      <td>No</td>\n",
       "      <td>35.104023</td>\n",
       "      <td>60.971333</td>\n",
       "      <td>High_spent_Large_value_payments</td>\n",
       "      <td>479.866228</td>\n",
       "      <td>Poor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0x25fea</th>\n",
       "      <td>CUS_0x942c</td>\n",
       "      <td>May</td>\n",
       "      <td>25</td>\n",
       "      <td>078</td>\n",
       "      <td>Mechanic</td>\n",
       "      <td>39628.99</td>\n",
       "      <td>3359.415833</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>Auto Loan, and Student Loan</td>\n",
       "      <td>18</td>\n",
       "      <td>7.0</td>\n",
       "      <td>11.50</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>502.38</td>\n",
       "      <td>40.565631</td>\n",
       "      <td>31 Years and 7 Months</td>\n",
       "      <td>No</td>\n",
       "      <td>35.104023</td>\n",
       "      <td>54.185950</td>\n",
       "      <td>High_spent_Medium_value_payments</td>\n",
       "      <td>496.651610</td>\n",
       "      <td>Poor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0x25feb</th>\n",
       "      <td>CUS_0x942c</td>\n",
       "      <td>June</td>\n",
       "      <td>25</td>\n",
       "      <td>078</td>\n",
       "      <td>Mechanic</td>\n",
       "      <td>39628.99</td>\n",
       "      <td>3359.415833</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>5729</td>\n",
       "      <td>2</td>\n",
       "      <td>Auto Loan, and Student Loan</td>\n",
       "      <td>27</td>\n",
       "      <td>6.0</td>\n",
       "      <td>11.50</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Good</td>\n",
       "      <td>502.38</td>\n",
       "      <td>41.255522</td>\n",
       "      <td>31 Years and 8 Months</td>\n",
       "      <td>No</td>\n",
       "      <td>35.104023</td>\n",
       "      <td>24.028477</td>\n",
       "      <td>High_spent_Large_value_payments</td>\n",
       "      <td>516.809083</td>\n",
       "      <td>Poor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0x25fec</th>\n",
       "      <td>CUS_0x942c</td>\n",
       "      <td>July</td>\n",
       "      <td>25</td>\n",
       "      <td>078</td>\n",
       "      <td>Mechanic</td>\n",
       "      <td>39628.99</td>\n",
       "      <td>3359.415833</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>Auto Loan, and Student Loan</td>\n",
       "      <td>20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11.50</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Good</td>\n",
       "      <td>502.38</td>\n",
       "      <td>33.638208</td>\n",
       "      <td>31 Years and 9 Months</td>\n",
       "      <td>No</td>\n",
       "      <td>35.104023</td>\n",
       "      <td>251.672582</td>\n",
       "      <td>Low_spent_Large_value_payments</td>\n",
       "      <td>319.164979</td>\n",
       "      <td>Good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0x25fed</th>\n",
       "      <td>CUS_0x942c</td>\n",
       "      <td>August</td>\n",
       "      <td>25</td>\n",
       "      <td>078</td>\n",
       "      <td>Mechanic</td>\n",
       "      <td>39628.99</td>\n",
       "      <td>3359.415833</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>Auto Loan, and Student Loan</td>\n",
       "      <td>18</td>\n",
       "      <td>6.0</td>\n",
       "      <td>11.50</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Good</td>\n",
       "      <td>502.38</td>\n",
       "      <td>34.192463</td>\n",
       "      <td>31 Years and 10 Months</td>\n",
       "      <td>No</td>\n",
       "      <td>35.104023</td>\n",
       "      <td>167.163865</td>\n",
       "      <td>NaN</td>\n",
       "      <td>393.673696</td>\n",
       "      <td>Poor</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100000 rows Ã— 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Customer_ID     Month  Age SSN_Area_Code Occupation  Annual_Income  Monthly_Inhand_Salary  Num_Bank_Accounts  Num_Credit_Card  Interest_Rate  NumofLoan                                                         Type_of_Loan  Delay_from_due_date  NumofDelayedPayment  ChangedCreditLimit  NumCreditInquiries CreditMix  OutstandingDebt  CreditUtilizationRatio      Credit_History_Age Payment_of_Min_Amount  TotalEMIpermonth  Amountinvestedmonthly                 Payment_Behaviour  MonthlyBalance Credit_Score\n",
       "ID                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             \n",
       "0x1602    CUS_0xd40   January   23           821  Scientist       19114.12            1824.843333                  3                4              3          4  Auto Loan, Credit-Builder Loan, Personal Loan, and Home Equity Loan                    3                  7.0               11.27                 4.0       NaN           809.98               26.822620   22 Years and 1 Months                    No         49.574949              80.415295   High_spent_Small_value_payments      312.494089         Good\n",
       "0x1603    CUS_0xd40  February   23           821  Scientist       19114.12                    NaN                  3                4              3          4  Auto Loan, Credit-Builder Loan, Personal Loan, and Home Equity Loan                   -1                  NaN               11.27                 4.0      Good           809.98               31.944960                     NaN                    No         49.574949             118.280222    Low_spent_Large_value_payments      284.629163         Good\n",
       "0x1604    CUS_0xd40     March  500           821  Scientist       19114.12                    NaN                  3                4              3          4  Auto Loan, Credit-Builder Loan, Personal Loan, and Home Equity Loan                    3                  7.0                 NaN                 4.0      Good           809.98               28.609352   22 Years and 3 Months                    No         49.574949              81.699521   Low_spent_Medium_value_payments      331.209863         Good\n",
       "0x1605    CUS_0xd40     April   23           821  Scientist       19114.12                    NaN                  3                4              3          4  Auto Loan, Credit-Builder Loan, Personal Loan, and Home Equity Loan                    5                  4.0                6.27                 4.0      Good           809.98               31.377862   22 Years and 4 Months                    No         49.574949             199.458074    Low_spent_Small_value_payments      223.451310         Good\n",
       "0x1606    CUS_0xd40       May   23           821  Scientist       19114.12            1824.843333                  3                4              3          4  Auto Loan, Credit-Builder Loan, Personal Loan, and Home Equity Loan                    6                  NaN               11.27                 4.0      Good           809.98               24.797347   22 Years and 5 Months                    No         49.574949              41.420153  High_spent_Medium_value_payments      341.489231         Good\n",
       "...             ...       ...  ...           ...        ...            ...                    ...                ...              ...            ...        ...                                                                  ...                  ...                  ...                 ...                 ...       ...              ...                     ...                     ...                   ...               ...                    ...                               ...             ...          ...\n",
       "0x25fe9  CUS_0x942c     April   25           078   Mechanic       39628.99            3359.415833                  4                6              7          2                                          Auto Loan, and Student Loan                   23                  7.0               11.50                 3.0       NaN           502.38               34.663572   31 Years and 6 Months                    No         35.104023              60.971333   High_spent_Large_value_payments      479.866228         Poor\n",
       "0x25fea  CUS_0x942c       May   25           078   Mechanic       39628.99            3359.415833                  4                6              7          2                                          Auto Loan, and Student Loan                   18                  7.0               11.50                 3.0       NaN           502.38               40.565631   31 Years and 7 Months                    No         35.104023              54.185950  High_spent_Medium_value_payments      496.651610         Poor\n",
       "0x25feb  CUS_0x942c      June   25           078   Mechanic       39628.99            3359.415833                  4                6           5729          2                                          Auto Loan, and Student Loan                   27                  6.0               11.50                 3.0      Good           502.38               41.255522   31 Years and 8 Months                    No         35.104023              24.028477   High_spent_Large_value_payments      516.809083         Poor\n",
       "0x25fec  CUS_0x942c      July   25           078   Mechanic       39628.99            3359.415833                  4                6              7          2                                          Auto Loan, and Student Loan                   20                  NaN               11.50                 3.0      Good           502.38               33.638208   31 Years and 9 Months                    No         35.104023             251.672582    Low_spent_Large_value_payments      319.164979         Good\n",
       "0x25fed  CUS_0x942c    August   25           078   Mechanic       39628.99            3359.415833                  4                6              7          2                                          Auto Loan, and Student Loan                   18                  6.0               11.50                 3.0      Good           502.38               34.192463  31 Years and 10 Months                    No         35.104023             167.163865                               NaN      393.673696         Poor\n",
       "\n",
       "[100000 rows x 26 columns]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove non-digits from age column and convert into numeric\n",
    "data['Age'] = data['Age'].str.replace(r'[^0-9]+', '', regex=True)\n",
    "data['Age'] = pd.to_numeric(data['Age'])\n",
    "\n",
    "# Drop name column\n",
    "data = data.drop(columns=['Name'])\n",
    "\n",
    "# Leave only area code for SSN\n",
    "data['SSN'] = data['SSN'].str.slice(stop=3)\n",
    "data = data.rename(columns = {'SSN': 'SSN_Area_Code'})\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def process_loan_type_entry(entry):\n",
    "    if not isinstance(entry, float):\n",
    "        loan_types_split = []\n",
    "        type_list = entry.replace(' and ', ' ')\n",
    "        type_list = type_list.split(', ')\n",
    "        for loan_type in type_list:\n",
    "            loan_types_split.append('Loan_Type_' + loan_type.strip().replace(' ', '_').replace('-', '_'))\n",
    "        return loan_types_split\n",
    "    return np.nan\n",
    "\n",
    "# Split loan types and reformat the strings\n",
    "\n",
    "no_nans = data.dropna()\n",
    "loan_values = no_nans['Type_of_Loan'].unique()\n",
    "\n",
    "loan_types = []\n",
    "for entry in loan_values:\n",
    "    loan_types += process_loan_type_entry(entry)\n",
    "\n",
    "loan_types_columns = set(loan_types)\n",
    "loan_types_columns = list(loan_types_columns)\n",
    "\n",
    "\n",
    "# Create columns and add to dataframe\n",
    "\n",
    "def columns_count_occurrences(column_names, list_to_count):\n",
    "    column_values = dict.fromkeys(column_names, 0)\n",
    "    if isinstance(list_to_count, float):    # nan\n",
    "        for key in column_values:\n",
    "            column_values[key] = np.nan\n",
    "    else:\n",
    "        for item in list_to_count:\n",
    "            column_values[item] += 1\n",
    "    return column_values\n",
    "\n",
    "\n",
    "data[loan_types_columns] = data.apply(lambda row: columns_count_occurrences(loan_types_columns, process_loan_type_entry(row['Type_of_Loan'])), axis='columns', result_type='expand')\n",
    "\n",
    "data = data.drop(columns = ['Type_of_Loan'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can we drop \"num of loan\"?\n",
    "\n",
    "data = data.drop(columns = ['NumofLoan'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert credit history to months\n",
    "\n",
    "import re\n",
    "\n",
    "def convert_age_to_months(age):\n",
    "    if isinstance(age, str):\n",
    "        list_of_numbers = re.findall(r'\\b\\d+\\b', age)\n",
    "        if (len(list_of_numbers) != 2):\n",
    "            print(list_of_numbers)\n",
    "            raise Exception('Incorrect age input')\n",
    "        years, months = int(list_of_numbers[0]), int(list_of_numbers[1])\n",
    "        total_months = years * 12 + months\n",
    "        return total_months\n",
    "    return np.nan\n",
    "\n",
    "data['Credit_History_Age_Months'] = data.apply(lambda row: convert_age_to_months(row['Credit_History_Age']), axis='columns', result_type='expand')\n",
    "data = data.drop(columns=[\"Credit_History_Age\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dummify(df: DataFrame, vars_to_dummify: list[str]) -> DataFrame:\n",
    "    other_vars: list[str] = [c for c in df.columns if not c in vars_to_dummify]\n",
    "\n",
    "    enc = OneHotEncoder(\n",
    "        handle_unknown=\"ignore\", sparse_output=False, dtype=\"bool\", drop=\"if_binary\"\n",
    "    )\n",
    "    trans: ndarray = enc.fit_transform(df[vars_to_dummify])\n",
    "\n",
    "    new_vars: ndarray = enc.get_feature_names_out(vars_to_dummify)\n",
    "\n",
    "    dummy = DataFrame(trans, columns=new_vars, index=df.index)\n",
    "\n",
    "    final_df: DataFrame = concat([df[other_vars], dummy], axis=1)\n",
    "    return final_df\n",
    "\n",
    "def encode_cyclic_variables(data: DataFrame, vars: list[str]):\n",
    "    _data = data\n",
    "    for v in vars:\n",
    "        x_max: float = max(data[v])\n",
    "        _data[v + \"_sin\"] = data[v].apply(lambda x: round(sin(2 * pi * x / x_max), 5))\n",
    "        _data[v + \"_cos\"] = data[v].apply(lambda x: round(cos(2 * pi * x / x_max), 5))\n",
    "    return _data\n",
    "\n",
    "\n",
    "# Substitutions\n",
    "credit_mix_enc: dict[str, int] = {\"Good\": 2, \"Standard\": 1, \"Bad\": 0}\n",
    "credit_score_enc: dict[str, int] = {\"Good\": 1, \"Poor\": 0}\n",
    "payment_min_amount_enc: dict[str, int] = {\"Yes\": 2, \"NM\": 1, \"No\": 0}\n",
    "month_val: dict[str, float] = {\n",
    "    \"January\": 0,\n",
    "    \"February\": pi / 4,\n",
    "    \"March\": 2 * pi / 4,\n",
    "    \"April\": 3 * pi / 4,\n",
    "    \"May\": pi,\n",
    "    \"June\": - 3 * pi / 4,\n",
    "    \"July\": - 2 * pi / 4,\n",
    "    \"August\": - pi / 4\n",
    "}\n",
    "\n",
    "encoding: dict[str, dict[str, int]] = {\n",
    "    \"CreditMix\": credit_mix_enc,\n",
    "    \"Month\": month_val,\n",
    "    \"Credit_Score\": credit_score_enc,\n",
    "    \"Payment_of_Min_Amount\": payment_min_amount_enc\n",
    "}\n",
    "\n",
    "\n",
    "# Replace values on dataframe\n",
    "\n",
    "data = data.replace(encoding, inplace=False)\n",
    "\n",
    "vars = [\"Occupation\"]\n",
    "data = dummify(data, vars)\n",
    "\n",
    "data['Customer_ID'] = data.apply(lambda row: int(row['Customer_ID'].replace('CUS_', ''), 16), axis='columns', result_type='expand')\n",
    "\n",
    "data = encode_cyclic_variables(data, [\"Month\"])\n",
    "data = data.drop(columns=['Month'])\n",
    "\n",
    "data[\"Payment_Behaviour_Spent\"] = data[\"Payment_Behaviour\"].map(lambda x: 0 if str(x).split('_')[0] == \"Low\" else 1 if str(x).split('_')[0] == \"High\" else np.nan)\n",
    "\n",
    "data[\"Payment_Behaviour_Value\"] = data[\"Payment_Behaviour\"].map(lambda x: np.nan if len(str(x).split('_')) != 5 else 0 if str(x).split('_')[2] == \"Small\" else 1 if str(x).split('_')[2] == \"Medium\" else 2)\n",
    "data = data.drop(columns=['Payment_Behaviour'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing value imputation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2nd approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed columns:\n",
      "[]\n",
      "(100000, 50)\n"
     ]
    }
   ],
   "source": [
    "# Phase 1: dropping MVs (records and variables)\n",
    "\n",
    "data_mvi_1 = data.copy()\n",
    "\n",
    "def mvi_by_dropping(\n",
    "    data: DataFrame, min_pct_per_variable: float = 0.1, min_pct_per_record: float = 0.0\n",
    ") -> DataFrame:\n",
    "    # Deleting variables\n",
    "    df: DataFrame = data.dropna(\n",
    "        axis=1, thresh=data.shape[0] * min_pct_per_variable, inplace=False\n",
    "    )\n",
    "    # Deleting records\n",
    "    df.dropna(axis=0, thresh=data.shape[1] * min_pct_per_record, inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "cols_before_drop = data_mvi_1.columns\n",
    "\n",
    "# \"thresh=N requires that a column has at least N non-NaNs to survive\"\n",
    "data_mvi_1 = mvi_by_dropping(data_mvi_1, min_pct_per_variable=0.1, min_pct_per_record=0.1)\n",
    "\n",
    "print('Removed columns:')\n",
    "print(list(set(cols_before_drop) - set(data_mvi_1.columns)))\n",
    "print(data_mvi_1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 2: filling MVs\n",
    "\n",
    "from numpy import ndarray\n",
    "from pandas import concat\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from dslabs_functions import get_variable_types, mvi_by_filling\n",
    "\n",
    "def mvi_by_filling_per_column(data: DataFrame, cols: list, strategy: str = \"mean\") -> Series:\n",
    "    # Possible strategies: \"mean\", \"median\", \"most_frequent\"\n",
    "    if strategy != \"knn\":\n",
    "        imp = SimpleImputer(strategy=strategy, copy=True)\n",
    "        df = DataFrame(\n",
    "            imp.fit_transform(data[cols]),\n",
    "            columns=cols,\n",
    "        )\n",
    "        return df[cols]\n",
    "    else:\n",
    "        imp = KNNImputer(n_neighbors=5)\n",
    "        imp.fit(data)\n",
    "        ar: ndarray = imp.transform(data)\n",
    "        df = DataFrame(ar, columns=data.columns, index=data.index)\n",
    "        return df\n",
    "\n",
    "df_to_change = data.copy()\n",
    "\n",
    "df = mvi_by_filling_per_column(df_to_change, [], 'knn')\n",
    "\n",
    "knn_cols = ['SSN_Area_Code', 'CreditMix', 'Amountinvestedmonthly', 'MonthlyBalance']\n",
    "data_mvi_1[knn_cols] = df[knn_cols].copy()\n",
    "\n",
    "knn_cols_3 = ['SSN_Area_Code', 'CreditMix', 'Amountinvestedmonthly', 'MonthlyBalance', 'Monthly_Inhand_Salary',\n",
    "              'NumofDelayedPayment', 'ChangedCreditLimit', 'NumCreditInquiries', 'Credit_History_Age_Months', 'Payment_Behaviour_Spent', 'Payment_Behaviour_Value']\n",
    "data_mvi_3 = df.copy()\n",
    "data_mvi_3[knn_cols_3] = df[knn_cols_3]\n",
    "\n",
    "# ----\n",
    "\n",
    "median_cols = ['Monthly_Inhand_Salary', 'NumofDelayedPayment', 'ChangedCreditLimit', 'NumCreditInquiries', 'Credit_History_Age_Months']\n",
    "data_mvi_1[median_cols] = mvi_by_filling_per_column(data_mvi_1, median_cols, 'median').values\n",
    "\n",
    "mode_cols = ['Payment_Behaviour_Spent', 'Payment_Behaviour_Value']\n",
    "data_mvi_1[mode_cols] = mvi_by_filling_per_column(data_mvi_1, mode_cols, 'most_frequent').values\n",
    "\n",
    "# Type of loan\n",
    "data_mvi_1[loan_types_columns] = mvi_by_filling_per_column(data_mvi_1, loan_types_columns, 'most_frequent').values\n",
    "\n",
    "# Occupations\n",
    "occupation_columns = ['Occupation_Accountant', 'Occupation_Architect', 'Occupation_Developer', 'Occupation_Doctor',\n",
    "                      'Occupation_Engineer', 'Occupation_Entrepreneur', 'Occupation_Journalist', 'Occupation_Lawyer',\n",
    "                       'Occupation_Manager', 'Occupation_Mechanic', 'Occupation_Media_Manager', 'Occupation_Musician',\n",
    "                       'Occupation_Scientist', 'Occupation_Teacher', 'Occupation_Writer']\n",
    "\n",
    "occupation_perc = []\n",
    "for occ in occupation_columns:\n",
    "    occupation_count = (data_mvi_1[occ] == True).sum()\n",
    "    total_occupation = (data_mvi_1['Occupation_nan'] == False).sum()\n",
    "    occupation_perc.append(occupation_count / total_occupation)\n",
    "\n",
    "def choose_from_probability(column_names, probabilities):\n",
    "    column_values = dict.fromkeys(column_names, False)\n",
    "    chosen = np.random.choice(column_names, p=probabilities)\n",
    "    column_values[chosen] = True\n",
    "    return column_values\n",
    "\n",
    "data_mvi_1[occupation_columns] = data_mvi_1.apply(lambda row: row[occupation_columns] if row['Occupation_nan'] == False else choose_from_probability(occupation_columns, occupation_perc), axis='columns', result_type='expand')\n",
    "data_mvi_1 = data_mvi_1.drop(columns = ['Occupation_nan'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_mvi_3[occupation_columns] = data_mvi_3.apply(lambda row: row[occupation_columns] if row['Occupation_nan'] == False else choose_from_probability(occupation_columns, occupation_perc), axis='columns', result_type='expand')\n",
    "data_mvi_3 = data_mvi_3.drop(columns = ['Occupation_nan'])\n",
    "data_mvi_3[loan_types_columns] = mvi_by_filling_per_column(data_mvi_3, loan_types_columns, 'most_frequent').values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparison between the two approaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import ndarray\n",
    "from pandas import DataFrame, read_csv\n",
    "from matplotlib.pyplot import savefig, show, figure\n",
    "from dslabs_functions import plot_multibar_chart, CLASS_EVAL_METRICS, run_NB, run_KNN\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "file_tag = 'credit_score_mvi_1'\n",
    "target = 'Credit_Score'\n",
    "y = data_mvi_1[target]\n",
    "X = data_mvi_1.drop(columns = target)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "train = pd.concat([X_train, y_train], axis=1)\n",
    "test = pd.concat([X_test, y_test], axis=1)\n",
    "\n",
    "\n",
    "figure()\n",
    "eval: dict[str, list] = evaluate_approach(train, test, target=target, metric=\"recall\")\n",
    "plot_multibar_chart(\n",
    "    [\"NB\", \"KNN\"], eval, title=f\"{file_tag} evaluation\", percentage=True\n",
    ")\n",
    "savefig(f\"images/{file_tag}_eval.png\")\n",
    "show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_tag = 'credit_score_mvi_3'\n",
    "target = 'Credit_Score'\n",
    "y = data_mvi_3[target]\n",
    "X = data_mvi_3.drop(columns = target)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "train = pd.concat([X_train, y_train], axis=1)\n",
    "test = pd.concat([X_test, y_test], axis=1)\n",
    "\n",
    "\n",
    "figure()\n",
    "eval: dict[str, list] = evaluate_approach(train, test, target=target, metric=\"recall\")\n",
    "plot_multibar_chart(\n",
    "    [\"NB\", \"KNN\"], eval, title=f\"{file_tag} evaluation\", percentage=True\n",
    ")\n",
    "savefig(f\"images/{file_tag}_eval.png\")\n",
    "show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outliers treatment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Original data: {data_mvi_3.shape}\")\n",
    "\n",
    "n_std: int = NR_STDEV\n",
    "numeric_vars: list[str] = get_variable_types(data_mvi_3)[\"numeric\"]\n",
    "\n",
    "for dataset in [\"mvi_1\", \"mvi_3\"]:\n",
    "# Alternative 1: Dropping Outliers\n",
    "\n",
    "    df: DataFrame = data_mvi_3.copy(deep=True) if dataset == 'mvi_3' else data_mvi_1.copy(deep=True)\n",
    "    file_tag = f'credit_score_outliers_{dataset}'\n",
    "    target = 'Credit_Score'\n",
    "    if numeric_vars is not None:\n",
    "        \n",
    "        summary5: DataFrame = data_mvi_3[numeric_vars].describe()\n",
    "        for var in numeric_vars:\n",
    "            top_threshold, bottom_threshold = determine_outlier_thresholds_for_var(\n",
    "                summary5[var],\n",
    "                std_based=False\n",
    "            )\n",
    "            outliers: Series = df[(df[var] > top_threshold) | (df[var] < bottom_threshold)]\n",
    "            df.drop(outliers.index, axis=0, inplace=True)\n",
    "        print(f\"Data after dropping outliers: {df.shape}\")\n",
    "\n",
    "        \n",
    "        y = df[target]\n",
    "        X = df.drop(columns = target)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "        train = pd.concat([X_train, y_train], axis=1)\n",
    "        test = pd.concat([X_test, y_test], axis=1)\n",
    "\n",
    "        figure()\n",
    "        eval: dict[str, list] = evaluate_approach(train, test, target=target, metric=\"recall\")\n",
    "        plot_multibar_chart(\n",
    "            [\"NB\", \"KNN\"], eval, title=f\"{file_tag}_drop_iqr evaluation\", percentage=True\n",
    "        )\n",
    "        savefig(f\"images/{file_tag}_drop_iqr_eval.png\")\n",
    "        show()\n",
    "    else:\n",
    "        print(\"There are no numeric variables\")\n",
    "\n",
    "    # Alternative 2: Replacing outliers with fixed value\n",
    "\n",
    "    df: DataFrame = data_mvi_3.copy(deep=True) if dataset == 'mvi_3' else data_mvi_1.copy(deep=True)\n",
    "    if [] != numeric_vars:\n",
    "        for var in numeric_vars:\n",
    "            top, bottom = determine_outlier_thresholds_for_var(summary5[var], std_based=False)\n",
    "            median: float = df[var].median()\n",
    "            df[var] = df[var].apply(lambda x: median if x > top or x < bottom else x)\n",
    "        print(\"Data after replacing outliers:\", df.shape)\n",
    "\n",
    "        y = df[target]\n",
    "        X = df.drop(columns = target)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "        train = pd.concat([X_train, y_train], axis=1)\n",
    "        test = pd.concat([X_test, y_test], axis=1)\n",
    "\n",
    "\n",
    "        figure()\n",
    "        eval: dict[str, list] = evaluate_approach(train, test, target=target, metric=\"recall\")\n",
    "        plot_multibar_chart(\n",
    "            [\"NB\", \"KNN\"], eval, title=f\"{file_tag}_replace_iqr evaluation\", percentage=True\n",
    "        )\n",
    "        savefig(f\"images/{file_tag}_replace_iqr_eval.png\")\n",
    "        show()\n",
    "    else:\n",
    "        print(\"There are no numeric variables\")\n",
    "\n",
    "\n",
    "    # Alternative 3: Truncating outliers\n",
    "    df: DataFrame = data_mvi_3.copy(deep=True) if dataset == 'mvi_3' else data_mvi_1.copy(deep=True)\n",
    "    if [] != numeric_vars:\n",
    "        for var in numeric_vars:\n",
    "            top, bottom = determine_outlier_thresholds_for_var(summary5[var], std_based=False)\n",
    "            df[var] = df[var].apply(\n",
    "                lambda x: top if x > top else bottom if x < bottom else x\n",
    "            )\n",
    "        print(\"Data after truncating outliers:\", df.shape)\n",
    "\n",
    "        y = df[target]\n",
    "        X = df.drop(columns = target)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "        train = pd.concat([X_train, y_train], axis=1)\n",
    "        test = pd.concat([X_test, y_test], axis=1)\n",
    "\n",
    "\n",
    "        figure()\n",
    "        eval: dict[str, list] = evaluate_approach(train, test, target=target, metric=\"recall\")\n",
    "        plot_multibar_chart(\n",
    "            [\"NB\", \"KNN\"], eval, title=f\"{file_tag}_truncate_iqr evaluation\", percentage=True\n",
    "        )\n",
    "        savefig(f\"images/{file_tag}_trunc_iqr_eval.png\")\n",
    "        show()\n",
    "    else:\n",
    "        print(\"There are no numeric variables\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Original data: {data_mvi_3.shape}\")\n",
    "\n",
    "n_std: int = NR_STDEV\n",
    "numeric_vars: list[str] = get_variable_types(data_mvi_3)[\"numeric\"]\n",
    "\n",
    "\n",
    "for dataset in [\"mvi_1\", \"mvi_3\"]:\n",
    "# Alternative 1: Dropping Outliers\n",
    "\n",
    "    df: DataFrame = data_mvi_3.copy(deep=True) if dataset == 'mvi_3' else data_mvi_1.copy(deep=True)\n",
    "    file_tag = f'credit_score_outliers_{dataset}'\n",
    "    target = 'Credit_Score'\n",
    "    if numeric_vars is not None:\n",
    "        \n",
    "        summary5: DataFrame = data_mvi_3[numeric_vars].describe()\n",
    "        for var in numeric_vars:\n",
    "            top_threshold, bottom_threshold = determine_outlier_thresholds_for_var(\n",
    "                summary5[var],\n",
    "            )\n",
    "            outliers: Series = df[(df[var] > top_threshold) | (df[var] < bottom_threshold)]\n",
    "            df.drop(outliers.index, axis=0, inplace=True)\n",
    "        print(f\"Data after dropping outliers: {df.shape}\")\n",
    "\n",
    "        \n",
    "        y = df[target]\n",
    "        X = df.drop(columns = target)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "        train = pd.concat([X_train, y_train], axis=1)\n",
    "        test = pd.concat([X_test, y_test], axis=1)\n",
    "\n",
    "\n",
    "        figure()\n",
    "        eval: dict[str, list] = evaluate_approach(train, test, target=target, metric=\"recall\")\n",
    "        plot_multibar_chart(\n",
    "            [\"NB\", \"KNN\"], eval, title=f\"{file_tag}_drop_std_dev evaluation\", percentage=True\n",
    "        )\n",
    "        savefig(f\"images/{file_tag}_drop_std_dev_eval.png\")\n",
    "        show()\n",
    "    else:\n",
    "        print(\"There are no numeric variables\")\n",
    "\n",
    "    # Alternative 2: Replacing outliers with fixed value\n",
    "\n",
    "    df: DataFrame = data_mvi_3.copy(deep=True) if dataset == 'mvi_3' else data_mvi_1.copy(deep=True)\n",
    "    if [] != numeric_vars:\n",
    "        for var in numeric_vars:\n",
    "            top, bottom = determine_outlier_thresholds_for_var(summary5[var])\n",
    "            median: float = df[var].median()\n",
    "            df[var] = df[var].apply(lambda x: median if x > top or x < bottom else x)\n",
    "        print(\"Data after replacing outliers:\", df.shape)\n",
    "\n",
    "        y = df[target]\n",
    "        X = df.drop(columns = target)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "        train = pd.concat([X_train, y_train], axis=1)\n",
    "        test = pd.concat([X_test, y_test], axis=1)\n",
    "\n",
    "\n",
    "        figure()\n",
    "        eval: dict[str, list] = evaluate_approach(train, test, target=target, metric=\"recall\")\n",
    "        plot_multibar_chart(\n",
    "            [\"NB\", \"KNN\"], eval, title=f\"{file_tag}_replace_std_dev evaluation\", percentage=True\n",
    "        )\n",
    "        savefig(f\"images/{file_tag}_replace_std_dev_eval.png\")\n",
    "        show()\n",
    "    else:\n",
    "        print(\"There are no numeric variables\")\n",
    "\n",
    "\n",
    "    # Alternative 3: Truncating outliers\n",
    "    df: DataFrame = data_mvi_3.copy(deep=True) if dataset == 'mvi_3' else data_mvi_1.copy(deep=True)\n",
    "    if [] != numeric_vars:\n",
    "        for var in numeric_vars:\n",
    "            top, bottom = determine_outlier_thresholds_for_var(summary5[var], std_based=False)\n",
    "            df[var] = df[var].apply(\n",
    "                lambda x: top if x > top else bottom if x < bottom else x\n",
    "            )\n",
    "        print(\"Data after truncating outliers:\", df.shape)\n",
    "\n",
    "        y = df[target]\n",
    "        X = df.drop(columns = target)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "        train = pd.concat([X_train, y_train], axis=1)\n",
    "        test = pd.concat([X_test, y_test], axis=1)\n",
    "\n",
    "\n",
    "        figure()\n",
    "        eval: dict[str, list] = evaluate_approach(train, test, target=target, metric=\"recall\")\n",
    "        plot_multibar_chart(\n",
    "            [\"NB\", \"KNN\"], eval, title=f\"{file_tag}_truncate_std_dev evaluation\", percentage=True\n",
    "        )\n",
    "        savefig(f\"images/{file_tag}_trunc_std_dev_eval.png\")\n",
    "        show()\n",
    "    else:\n",
    "        print(\"There are no numeric variables\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Assign the best to the best dataset mvi\n",
    "\n",
    "data_mvi_3_outliers_replace = data_mvi_3.copy()\n",
    "summary5: DataFrame = data_mvi_3_outliers_replace[numeric_vars].describe()\n",
    "if [] != numeric_vars:\n",
    "    for var in numeric_vars:\n",
    "        top, bottom = determine_outlier_thresholds_for_var(summary5[var], std_based=False)\n",
    "        median: float = data_mvi_3_outliers_replace[var].median()\n",
    "        data_mvi_3[var] = data_mvi_3_outliers_replace[var].apply(lambda x: median if x > top or x < bottom else x)\n",
    "    print(\"Data after replacing outliers:\", data_mvi_3_outliers_replace.shape)\n",
    "    print(data_mvi_3_outliers_replace.head())\n",
    "    \n",
    "else:\n",
    "    print(\"There are no numeric variables\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Alternative 1: Standard Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import read_csv, DataFrame, Series\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "#Alternative 1: Standard Scalar\n",
    "\n",
    "data_to_eval = data_mvi_3_outliers_replace.copy()\n",
    "\n",
    "target = \"Credit_Score\"\n",
    "file_tag = f'credit_score_scaling_mvi_3'\n",
    "\n",
    "vars: list[str] = data_to_eval.columns.to_list()\n",
    "target_data: Series = data_to_eval.pop(target)\n",
    "\n",
    "transf: StandardScaler = StandardScaler(with_mean=True, with_std=True, copy=True).fit(\n",
    "    data_to_eval\n",
    ")\n",
    "df_zscore = DataFrame(transf.transform(data_to_eval), index=data_to_eval.index)\n",
    "vars.remove(target)\n",
    "df_zscore.columns = vars\n",
    "df_zscore[target] = target_data\n",
    "\n",
    "\n",
    "# Charts for evaluation:\n",
    "\n",
    "y = df_zscore[target]\n",
    "X = df_zscore.drop(columns = target)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "train = pd.concat([X_train, y_train], axis=1)\n",
    "test = pd.concat([X_test, y_test], axis=1)\n",
    "figure()\n",
    "eval: dict[str, list] = evaluate_approach(train, test, target=target, metric=\"recall\")\n",
    "plot_multibar_chart(\n",
    "    [\"NB\", \"KNN\"], eval, title=f\"{file_tag}_zscore evaluation\", percentage=True\n",
    ")\n",
    "savefig(f\"images/{file_tag}_zscore_eval.png\")\n",
    "show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Alternative 2: MinMax Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "data_to_eval = data_mvi_3_outliers_replace.copy()\n",
    "\n",
    "vars: list[str] = data_to_eval.columns.to_list()\n",
    "target_data: Series = data_to_eval.pop(target)\n",
    "\n",
    "transf: MinMaxScaler = MinMaxScaler(feature_range=(0, 1), copy=True).fit(data_to_eval)\n",
    "df_minmax = DataFrame(transf.transform(data_to_eval), index=data_to_eval.index)\n",
    "vars.remove(target)\n",
    "df_minmax.columns = vars\n",
    "df_minmax[target] = target_data\n",
    "\n",
    "\n",
    "# Charts for evaluation:\n",
    "\n",
    "y = df_minmax[target]\n",
    "X = df_minmax.drop(columns = target)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "train = pd.concat([X_train, y_train], axis=1)\n",
    "test = pd.concat([X_test, y_test], axis=1)\n",
    "figure()\n",
    "eval: dict[str, list] = evaluate_approach(train, test, target=target, metric=\"recall\")\n",
    "plot_multibar_chart(\n",
    "    [\"NB\", \"KNN\"], eval, title=f\"{file_tag}_minmax evaluation\", percentage=True\n",
    ")\n",
    "savefig(f\"images/{file_tag}_minmax_eval.png\")\n",
    "show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.pyplot import subplots, show\n",
    "\n",
    "# Visualize boxplots real quick...\n",
    "\n",
    "data_to_eval = data_mvi_3_outliers_replace.copy()\n",
    "vars: list[str] = data_to_eval.columns.to_list()\n",
    "target_data: Series = data_to_eval.pop(target)\n",
    "transf: MinMaxScaler = MinMaxScaler(feature_range=(0, 1), copy=True).fit(data_to_eval)\n",
    "df_minmax = DataFrame(transf.transform(data_to_eval), index=data_to_eval.index)\n",
    "df_zscore[target] = target_data\n",
    "df_zscore.columns = vars\n",
    "data_to_eval = data_mvi_3_outliers_replace.copy()\n",
    "vars: list[str] = data_to_eval.columns.to_list()\n",
    "target_data: Series = data_to_eval.pop(target)\n",
    "transf: MinMaxScaler = MinMaxScaler(feature_range=(0, 1), copy=True).fit(data_to_eval)\n",
    "df_minmax = DataFrame(transf.transform(data_to_eval), index=data_to_eval.index)\n",
    "df_minmax[target] = target_data\n",
    "df_minmax.columns = vars\n",
    "\n",
    "\n",
    "fig, axs = subplots(1, 3, figsize=(20, 10), squeeze=False)\n",
    "axs[0, 1].set_title(\"Original data\")\n",
    "data.boxplot(ax=axs[0, 0])\n",
    "axs[0, 0].set_title(\"Z-score normalization\")\n",
    "df_zscore.boxplot(ax=axs[0, 1])\n",
    "axs[0, 2].set_title(\"MinMax normalization\")\n",
    "df_minmax.boxplot(ax=axs[0, 2])\n",
    "show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assign best result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_after_scaling = data_mvi_3_outliers_replace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Balancing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Before balancing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import read_csv, concat, DataFrame, Series\n",
    "from matplotlib.pyplot import figure, show\n",
    "from dslabs_functions import plot_bar_chart\n",
    "\n",
    "data_to_balance = data_after_scaling.copy()\n",
    "\n",
    "target_count: Series = data_to_balance[target].value_counts()\n",
    "positive_class = target_count.idxmin()\n",
    "negative_class = target_count.idxmax()\n",
    "\n",
    "print(\"Minority class=\", positive_class, \":\", target_count[positive_class])\n",
    "print(\"Majority class=\", negative_class, \":\", target_count[negative_class])\n",
    "print(\n",
    "    \"Proportion:\",\n",
    "    round(target_count[positive_class] / target_count[negative_class], 2),\n",
    "    \": 1\",\n",
    ")\n",
    "values: dict[str, list] = {\n",
    "    \"Original\": [target_count[positive_class], target_count[negative_class]]\n",
    "}\n",
    "\n",
    "figure()\n",
    "plot_bar_chart(\n",
    "    target_count.index.to_list(), target_count.to_list(), title=\"Class balance\"\n",
    ")\n",
    "show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_positives: Series = data_to_balance[data_to_balance[target] == positive_class]\n",
    "df_negatives: Series = data_to_balance[data_to_balance[target] == negative_class]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Approach 1: Undersampling\n",
    "\n",
    "With a huge dataset, and consequently a considerable number of positve records, we can use an undersampling strategy, keeping the positive records and sampling the negative ones to balance the final distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_neg_sample: DataFrame = DataFrame(df_negatives.sample(len(df_positives)))\n",
    "df_under: DataFrame = concat([df_positives, df_neg_sample], axis=0)\n",
    "\n",
    "print(\"Minority class=\", positive_class, \":\", len(df_positives))\n",
    "print(\"Majority class=\", negative_class, \":\", len(df_neg_sample))\n",
    "print(\"Proportion:\", round(len(df_positives) / len(df_neg_sample), 2), \": 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate\n",
    "\n",
    "data_to_eval = df_under.copy()\n",
    "file_tag = f'credit_score_balancing_mvi_3_'\n",
    "\n",
    "y = data_to_eval[target]\n",
    "X = data_to_eval.drop(columns = target)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "train = pd.concat([X_train, y_train], axis=1)\n",
    "test = pd.concat([X_test, y_test], axis=1)\n",
    "figure()\n",
    "eval: dict[str, list] = evaluate_approach(train, test, target=target, metric=\"recall\")\n",
    "plot_multibar_chart(\n",
    "    [\"NB\", \"KNN\"], eval, title=f\"{file_tag}under evaluation\", percentage=True\n",
    ")\n",
    "savefig(f\"images/{file_tag}under_eval.png\")\n",
    "show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Approach 2: Oversampling\n",
    "\n",
    "In the presence of a small number of positive records, we need to apply oversampling, in order to create a larger set to support the training step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pos_sample: DataFrame = DataFrame(\n",
    "    df_positives.sample(len(df_negatives), replace=True)\n",
    ")\n",
    "df_over: DataFrame = concat([df_pos_sample, df_negatives], axis=0)\n",
    "\n",
    "print(\"Minority class=\", positive_class, \":\", len(df_pos_sample))\n",
    "print(\"Majority class=\", negative_class, \":\", len(df_negatives))\n",
    "print(\"Proportion:\", round(len(df_pos_sample) / len(df_negatives), 2), \": 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate\n",
    "\n",
    "data_to_eval = df_over.copy()\n",
    "file_tag = f'credit_score_balancing_mvi_3_'\n",
    "\n",
    "y = data_to_eval[target]\n",
    "X = data_to_eval.drop(columns = target)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "train = pd.concat([X_train, y_train], axis=1)\n",
    "test = pd.concat([X_test, y_test], axis=1)\n",
    "figure()\n",
    "eval: dict[str, list] = evaluate_approach(train, test, target=target, metric=\"recall\")\n",
    "plot_multibar_chart(\n",
    "    [\"NB\", \"KNN\"], eval, title=f\"{file_tag}over evaluation\", percentage=True\n",
    ")\n",
    "savefig(f\"images/{file_tag}over_eval.png\")\n",
    "show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Approach 3: SMOTE\n",
    "\n",
    "The oversample is created from the minority class, by artificially creating new records in the neighborhood of the positive records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import ndarray\n",
    "from pandas import Series\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "smote: SMOTE = SMOTE(sampling_strategy=\"minority\", random_state=RANDOM_STATE)\n",
    "y = data_to_balance.pop(target).values\n",
    "X: ndarray = data_to_balance.values\n",
    "smote_X, smote_y = smote.fit_resample(X, y)\n",
    "df_smote: DataFrame = concat([DataFrame(smote_X), DataFrame(smote_y)], axis=1)\n",
    "df_smote.columns = list(data_to_balance.columns) + [target]\n",
    "\n",
    "smote_target_count: Series = Series(smote_y).value_counts()\n",
    "print(\"Minority class=\", positive_class, \":\", smote_target_count[positive_class])\n",
    "print(\"Majority class=\", negative_class, \":\", smote_target_count[negative_class])\n",
    "print(\n",
    "    \"Proportion:\",\n",
    "    round(smote_target_count[positive_class] / smote_target_count[negative_class], 2),\n",
    "    \": 1\",\n",
    ")\n",
    "print(df_smote.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate\n",
    "\n",
    "data_to_eval = df_smote.copy()\n",
    "file_tag = f'credit_score_balancing_mvi_3_'\n",
    "\n",
    "y = data_to_eval[target]\n",
    "X = data_to_eval.drop(columns = target)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "train = pd.concat([X_train, y_train], axis=1)\n",
    "test = pd.concat([X_test, y_test], axis=1)\n",
    "figure()\n",
    "eval: dict[str, list] = evaluate_approach(train, test, target=target, metric=\"recall\")\n",
    "plot_multibar_chart(\n",
    "    [\"NB\", \"KNN\"], eval, title=f\"{file_tag}smote evaluation\", percentage=True\n",
    ")\n",
    "savefig(f\"images/{file_tag}smote_eval.png\")\n",
    "show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_after_scaling.to_csv('csv-results/credit-score-after-preparation.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
