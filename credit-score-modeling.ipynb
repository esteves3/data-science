{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dslabs_functions import get_variable_types\n",
    "from seaborn import heatmap\n",
    "from dslabs_functions import HEIGHT, plot_multi_scatters_chart\n",
    "from matplotlib.pyplot import figure, subplots, savefig, show, gcf\n",
    "from dslabs_functions import plot_bar_chart\n",
    "from dslabs_functions import set_chart_labels\n",
    "from dslabs_functions import define_grid, HEIGHT\n",
    "from matplotlib.figure import Figure\n",
    "from numpy import ndarray\n",
    "from dslabs_functions import *\n",
    "from pandas import read_csv, DataFrame\n",
    "from numpy import log\n",
    "from pandas import Series\n",
    "from scipy.stats import norm, expon, lognorm\n",
    "from matplotlib.axes import Axes\n",
    "from dslabs_functions import plot_multiline_chart\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option(\"display.max_colwidth\", 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = \"csv-results/credit-score-after-preparation.csv\"\n",
    "file_tag = \"credit_score\"\n",
    "data: DataFrame = read_csv(results, index_col=\"ID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SINGULAR BOXPLOT\n",
    "\n",
    "variables_types: dict[str, list] = get_variable_types(data)\n",
    "numeric: list[str] = variables_types[\"numeric\"]\n",
    "\n",
    "rows: int\n",
    "cols: int\n",
    "rows, cols = define_grid(len(numeric))\n",
    "fig: Figure\n",
    "axs: ndarray\n",
    "\n",
    "fig, axs = subplots(\n",
    "    rows, cols, figsize=(cols * HEIGHT, rows * HEIGHT), squeeze=False\n",
    ")\n",
    "\n",
    "i, j = 0, 0\n",
    "for n in range(len(numeric)):\n",
    "    axs[i, j].set_title(\"Boxplot for %s\" % numeric[n])\n",
    "    axs[i, j].boxplot(data[numeric[n]].dropna().values)\n",
    "    i, j = (i + 1, 0) if (n + 1) % cols == 0 else (i, j + 1)\n",
    "savefig(f\"images/{file_tag}_single_boxplots_after_prep.png\", bbox_inches=\"tight\")\n",
    "show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IMPORTANT REMINDER:** Remember that balancing cannot be applied to the test dataset, and that the evaluation of the models has to be done over an independent dataset (test set)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# UNCOMMENT IF BALANCING IS APPLIED:\n",
    "# data_to_split = read_csv('csv-results/credit-score-before-balancing')\n",
    "\n",
    "data_to_split = data.copy()\n",
    "\n",
    "# Split train/test\n",
    "\n",
    "target = 'Credit_Score'\n",
    "y = data_to_split[target]\n",
    "X = data_to_split.drop(columns = target)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "train = pd.concat([X_train, y_train], axis=1)\n",
    "test = pd.concat([X_test, y_test], axis=1)\n",
    "\n",
    "labels = data.Credit_Score.unique()\n",
    "\n",
    "vars = data.columns.to_list()\n",
    "vars.remove('Credit_Score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trnX, trnY, tstX, tstY = X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approaches study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array, ndarray\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB\n",
    "from matplotlib.pyplot import figure, savefig, show\n",
    "from dslabs_functions import CLASS_EVAL_METRICS, DELTA_IMPROVE, plot_bar_chart\n",
    "\n",
    "\n",
    "def naive_Bayes_study(\n",
    "    trnX: ndarray, trnY: array, tstX: ndarray, tstY: array, metric: str = \"accuracy\"\n",
    ") -> tuple:\n",
    "    estimators: dict = {\n",
    "        \"GaussianNB\": GaussianNB(),\n",
    "        # \"MultinomialNB\": MultinomialNB(),\n",
    "        \"BernoulliNB\": BernoulliNB(),\n",
    "    }\n",
    "\n",
    "    xvalues: list = []\n",
    "    yvalues: list = []\n",
    "    best_model = None\n",
    "    best_params: dict = {\"name\": \"\", \"metric\": metric, \"params\": ()}\n",
    "    best_performance = 0\n",
    "    for clf in estimators:\n",
    "        xvalues.append(clf)\n",
    "        estimators[clf].fit(trnX, trnY)\n",
    "        prdY: array = estimators[clf].predict(tstX)\n",
    "        eval: float = CLASS_EVAL_METRICS[metric](tstY, prdY)\n",
    "        if eval - best_performance > DELTA_IMPROVE:\n",
    "            best_performance: float = eval\n",
    "            best_params[\"name\"] = clf\n",
    "            best_params[metric] = eval\n",
    "            best_model = estimators[clf]\n",
    "        yvalues.append(eval)\n",
    "        # print(f'NB {clf}')\n",
    "    plot_bar_chart(\n",
    "        xvalues,\n",
    "        yvalues,\n",
    "        title=f\"Naive Bayes Models ({metric})\",\n",
    "        ylabel=metric,\n",
    "        percentage=True,\n",
    "    )\n",
    "\n",
    "    return best_model, best_params\n",
    "\n",
    "\n",
    "eval_metric = 'accuracy'\n",
    "figure()\n",
    "best_model_acc, params_acc = naive_Bayes_study(trnX, trnY, tstX, tstY, eval_metric)\n",
    "savefig(f\"images/modeling/{file_tag}_nb_{eval_metric}_study.png\")\n",
    "show()\n",
    "\n",
    "'''\n",
    "eval_metric = 'recall'\n",
    "figure()\n",
    "best_model_recall, params_recall = naive_Bayes_study(trnX, trnY, tstX, tstY, eval_metric)\n",
    "savefig(f\"images/modeling/{file_tag}_nb_{eval_metric}_study.png\")\n",
    "show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance analysis\n",
    "\n",
    "from dslabs_functions import plot_evaluation_results\n",
    "\n",
    "eval_metric = 'accuracy'\n",
    "prd_trn: array = best_model_acc.predict(trnX)\n",
    "prd_tst: array = best_model_acc.predict(tstX)\n",
    "figure()\n",
    "plot_evaluation_results(params_acc, y_train, prd_trn, tstY, prd_tst, labels)\n",
    "savefig(f'images/modeling/{file_tag}_{params_acc[\"name\"]}_best_{params_acc[\"metric\"]}_eval.png')\n",
    "show()\n",
    "\n",
    "'''\n",
    "eval_metric = 'recall'\n",
    "prd_trn: array = best_model_recall.predict(trnX)\n",
    "prd_tst: array = best_model_recall.predict(tstX)\n",
    "figure()\n",
    "plot_evaluation_results(params_recall, y_train, prd_trn, tstY, prd_tst, labels)\n",
    "savefig(f'images/modeling/{file_tag}_{params_recall[\"name\"]}_best_{params_recall[\"metric\"]}_eval.png')\n",
    "show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "from numpy import array, ndarray\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from matplotlib.pyplot import figure, savefig, show\n",
    "from dslabs_functions import CLASS_EVAL_METRICS, DELTA_IMPROVE, plot_multiline_chart\n",
    "from dslabs_functions import read_train_test_from_files, plot_evaluation_results\n",
    "\n",
    "def knn_study(\n",
    "        trnX: ndarray, trnY: array, tstX: ndarray, tstY: array, k_max: int=19, lag: int=2, metric='accuracy'\n",
    "        ) -> tuple[KNeighborsClassifier | None, dict]:\n",
    "    dist: list[Literal['manhattan', 'euclidean', 'chebyshev']] = ['manhattan', 'euclidean', 'chebyshev']\n",
    "\n",
    "    kvalues: list[int] = [i for i in range(1, k_max+1, lag)]\n",
    "    best_model: KNeighborsClassifier | None = None\n",
    "    best_params: dict = {'name': 'KNN', 'metric': metric, 'params': ()}\n",
    "    best_performance: float = 0.0\n",
    "\n",
    "    values: dict[str, list] = {}\n",
    "    for d in dist:\n",
    "        y_tst_values: list = []\n",
    "        for k in kvalues:\n",
    "            clf = KNeighborsClassifier(n_neighbors=k, metric=d)\n",
    "            clf.fit(trnX, trnY)\n",
    "            prdY: array = clf.predict(tstX)\n",
    "            eval: float = CLASS_EVAL_METRICS[metric](tstY, prdY)\n",
    "            y_tst_values.append(eval)\n",
    "            if eval - best_performance > DELTA_IMPROVE:\n",
    "                best_performance: float = eval\n",
    "                best_params['params'] = (k, d)\n",
    "                best_model = clf\n",
    "            # print(f'KNN {d} k={k}')\n",
    "        values[d] = y_tst_values\n",
    "    print(f'KNN best with k={best_params['params'][0]} and {best_params['params'][1]}')\n",
    "    plot_multiline_chart(kvalues, values, title=f'KNN Models ({metric})', xlabel='k', ylabel=metric, percentage=True)\n",
    "\n",
    "    return best_model, best_params\n",
    "\n",
    "\n",
    "eval_metric = 'accuracy'\n",
    "figure()\n",
    "best_model, params = knn_study(trnX, trnY, tstX, tstY, k_max=25, metric=eval_metric)\n",
    "savefig(f'images/modeling/{file_tag}_knn_{eval_metric}_study.png')\n",
    "show()\n",
    "\n",
    "'''\n",
    "eval_metric = 'recall'\n",
    "figure()\n",
    "best_model_recall, params_recall = knn_study(trnX, trnY, tstX, tstY, k_max=25, metric=eval_metric)\n",
    "savefig(f'images/modeling/{file_tag}_knn_{eval_metric}_study.png')\n",
    "show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prd_trn: array = best_model.predict(trnX)\n",
    "prd_tst: array = best_model.predict(tstX)\n",
    "figure()\n",
    "plot_evaluation_results(params, trnY, prd_trn, tstY, prd_tst, labels)\n",
    "savefig(f'images/modeling/{file_tag}_knn_{params[\"name\"]}_best_{params[\"metric\"]}_eval.png')\n",
    "show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.pyplot import figure, savefig\n",
    "\n",
    "distance: Literal[\"manhattan\", \"euclidean\", \"chebyshev\"] = params[\"params\"][1]\n",
    "K_MAX = 25\n",
    "kvalues: list[int] = [i for i in range(1, K_MAX, 2)]\n",
    "y_tst_values: list = []\n",
    "y_trn_values: list = []\n",
    "acc_metric: str = \"accuracy\"\n",
    "for k in kvalues:\n",
    "    clf = KNeighborsClassifier(n_neighbors=k, metric=distance)\n",
    "    clf.fit(trnX, trnY)\n",
    "    prd_tst_Y: array = clf.predict(tstX)\n",
    "    prd_trn_Y: array = clf.predict(trnX)\n",
    "    y_tst_values.append(CLASS_EVAL_METRICS[acc_metric](tstY, prd_tst_Y))\n",
    "    y_trn_values.append(CLASS_EVAL_METRICS[acc_metric](trnY, prd_trn_Y))\n",
    "\n",
    "figure()\n",
    "plot_multiline_chart(\n",
    "    kvalues,\n",
    "    {\"Train\": y_trn_values, \"Test\": y_tst_values},\n",
    "    title=f\"KNN overfitting study for {distance}\",\n",
    "    xlabel=\"K\",\n",
    "    ylabel=str(eval_metric),\n",
    "    percentage=True,\n",
    ")\n",
    "savefig(f\"images/modeling/{file_tag}_knn_overfitting.png\")\n",
    "show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Trees\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "from numpy import array, ndarray\n",
    "from matplotlib.pyplot import figure, savefig, show\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from dslabs_functions import CLASS_EVAL_METRICS, DELTA_IMPROVE, read_train_test_from_files\n",
    "from dslabs_functions import plot_evaluation_results, plot_multiline_chart\n",
    "\n",
    "\n",
    "def trees_study(\n",
    "        trnX: ndarray, trnY: array, tstX: ndarray, tstY: array, d_max: int=10, lag:int=2, metric='accuracy'\n",
    "        ) -> tuple:\n",
    "    criteria: list[Literal['entropy', 'gini']] = ['entropy', 'gini']\n",
    "    depths: list[int] = [i for i in range(2, d_max+1, lag)]\n",
    "\n",
    "    best_model: DecisionTreeClassifier | None = None\n",
    "    best_params: dict = {'name': 'DT', 'metric': metric, 'params': ()}\n",
    "    best_performance: float = 0.0\n",
    "\n",
    "    values: dict = {}\n",
    "    for c in criteria:\n",
    "        y_tst_values: list[float] = []\n",
    "        for d in depths:\n",
    "            clf = DecisionTreeClassifier(max_depth=d, criterion=c, min_impurity_decrease=0)\n",
    "            clf.fit(trnX, trnY)\n",
    "            prdY: array = clf.predict(tstX)\n",
    "            eval: float = CLASS_EVAL_METRICS[metric](tstY, prdY)\n",
    "            y_tst_values.append(eval)\n",
    "            if eval - best_performance > DELTA_IMPROVE:\n",
    "                best_performance = eval\n",
    "                best_params['params'] = (c, d)\n",
    "                best_model = clf\n",
    "            # print(f'DT {c} and d={d}')\n",
    "        values[c] = y_tst_values\n",
    "    print(f'DT best with {best_params['params'][0]} and d={best_params['params'][1]}')\n",
    "    plot_multiline_chart(depths, values, title=f'DT Models ({metric})', xlabel='d', ylabel=metric, percentage=True)\n",
    "\n",
    "    return best_model, best_params\n",
    "\n",
    "\n",
    "eval_metric = 'accuracy'\n",
    "print(f'Train#={len(trnX)} Test#={len(tstX)}')\n",
    "print(f'Labels={labels}')\n",
    "\n",
    "figure()\n",
    "best_model, params = trees_study(trnX, trnY, tstX, tstY, d_max=25, metric=eval_metric)\n",
    "savefig(f'images/modeling/{file_tag}_dt_{eval_metric}_study.png')\n",
    "show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prd_trn: array = best_model.predict(trnX)\n",
    "prd_tst: array = best_model.predict(tstX)\n",
    "figure()\n",
    "plot_evaluation_results(params, trnY, prd_trn, tstY, prd_tst, labels)\n",
    "savefig(f'images/modeling/{file_tag}_dt_{params[\"name\"]}_best_{params[\"metric\"]}_eval.png')\n",
    "show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import export_graphviz\n",
    "from matplotlib.pyplot import imread, imshow, axis\n",
    "from subprocess import call\n",
    "\n",
    "'''\n",
    "tree_filename: str = f\"images/modeling/{file_tag}_dt_{eval_metric}_best_tree\"\n",
    "max_depth2show = 4\n",
    "st_labels: list[str] = [str(value) for value in labels]\n",
    "\n",
    "dot_data: str = export_graphviz(\n",
    "    best_model,\n",
    "    out_file=tree_filename + \".dot\",\n",
    "    max_depth=max_depth2show,\n",
    "    feature_names=vars,\n",
    "    class_names=st_labels,\n",
    "    filled=True,\n",
    "    rounded=True,\n",
    "    impurity=False,\n",
    "    special_characters=True,\n",
    "    precision=2,\n",
    ")\n",
    "# Convert to png\n",
    "call(\n",
    "    [\"dot\", \"-Tpng\", tree_filename + \".dot\", \"-o\", tree_filename + \".png\", \"-Gdpi=600\"]\n",
    ")\n",
    "\n",
    "figure(figsize=(14, 6))\n",
    "imshow(imread(tree_filename + \".png\"))\n",
    "axis(\"off\")\n",
    "show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print a simpler version of the tree:\n",
    "\n",
    "from sklearn.tree import plot_tree\n",
    "\n",
    "tree_filename: str = f\"images/modeling/{file_tag}_dt_{eval_metric}_best_tree\"\n",
    "max_depth2show = 3\n",
    "st_labels: list[str] = [str(value) for value in labels]\n",
    "\n",
    "figure(figsize=(20, 20))\n",
    "plot_tree(\n",
    "    best_model,\n",
    "    max_depth=max_depth2show,\n",
    "    feature_names=vars,\n",
    "    class_names=st_labels,\n",
    "    filled=True,\n",
    "    rounded=True,\n",
    "    impurity=False,\n",
    "    precision=2,\n",
    ")\n",
    "savefig(tree_filename + \"_simple.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variables relevancy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import argsort\n",
    "from dslabs_functions import plot_horizontal_bar_chart\n",
    "\n",
    "importances = best_model.feature_importances_\n",
    "indices: list[int] = argsort(importances)[::-1]\n",
    "elems: list[str] = []\n",
    "imp_values: list[float] = []\n",
    "for f in range(len(vars)):\n",
    "    elems += [vars[indices[f]]]\n",
    "    imp_values += [importances[indices[f]]]\n",
    "    print(f\"{f+1}. {elems[f]} ({importances[indices[f]]})\")\n",
    "\n",
    "figure(figsize=(6, 8))\n",
    "plot_horizontal_bar_chart(\n",
    "    elems,\n",
    "    imp_values,\n",
    "    title=\"Decision Tree variables importance\",\n",
    "    xlabel=\"importance\",\n",
    "    ylabel=\"variables\",\n",
    "    percentage=True,\n",
    ")\n",
    "savefig(f\"images/modeling/{file_tag}_dt_{eval_metric}_vars_ranking.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crit: Literal[\"entropy\", \"gini\"] = params[\"params\"][0]\n",
    "d_max = 25\n",
    "depths: list[int] = [i for i in range(2, d_max + 1, 1)]\n",
    "y_tst_values: list[float] = []\n",
    "y_trn_values: list[float] = []\n",
    "acc_metric = \"accuracy\"\n",
    "for d in depths:\n",
    "    clf = DecisionTreeClassifier(max_depth=d, criterion=crit, min_impurity_decrease=0)\n",
    "    clf.fit(trnX, trnY)\n",
    "    prd_tst_Y: array = clf.predict(tstX)\n",
    "    prd_trn_Y: array = clf.predict(trnX)\n",
    "    y_tst_values.append(CLASS_EVAL_METRICS[acc_metric](tstY, prd_tst_Y))\n",
    "    y_trn_values.append(CLASS_EVAL_METRICS[acc_metric](trnY, prd_trn_Y))\n",
    "\n",
    "figure()\n",
    "plot_multiline_chart(\n",
    "    depths,\n",
    "    {\"Train\": y_trn_values, \"Test\": y_tst_values},\n",
    "    title=f\"DT overfitting study for {crit}\",\n",
    "    xlabel=\"max_depth\",\n",
    "    ylabel=str(eval_metric),\n",
    "    percentage=True,\n",
    ")\n",
    "savefig(f\"images/modeling/{file_tag}_dt_{eval_metric}_overfitting.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
